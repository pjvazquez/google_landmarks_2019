{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone is busy downloading and there was no kernel that kind of does the initial exploration to see how the images look like and how would a simple work on this dataset. Hence I went forward and created one. This kernel does not save any file locally and just uses them on the fly during training using a keras generator. Time ranges around ~240secs for an epoch with total 1280 images on a ResNet50 (with very minimal number of classes). There may be a few mistakes, you never know.\n",
    "\n",
    "Happy Kaggling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import urllib\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.applications import ResNet50\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, \\\n",
    "    Lambda, MaxPooling2D, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def check_size(url):\n",
    "    r = requests.get(url, stream=True)\n",
    "    return int(r.headers['Content-Length'])\n",
    "\n",
    "def download_file(url, filename, bar=True):\n",
    "    \"\"\"\n",
    "    Helper method handling downloading large files from `url` to `filename`. Returns a pointer to `filename`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chunkSize = 1024\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            if bar:\n",
    "                pbar = tqdm( unit=\"B\", total=int( r.headers['Content-Length'] ) )\n",
    "            for chunk in r.iter_content(chunk_size=chunkSize): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    if bar: \n",
    "                        pbar.update (len(chunk))\n",
    "                    f.write(chunk)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "    \n",
    "def download_image_cv2_urllib(url):\n",
    "    \"\"\"\n",
    "    Modifying the url to download the 360p or 720p version actually slows it down. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        foo = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "        foo = cv2.imdecode(foo, cv2.IMREAD_COLOR)\n",
    "        foo = cv2.resize(foo,(128, 128), interpolation=cv2.INTER_AREA)\n",
    "        return foo\n",
    "    except:\n",
    "        return np.array([])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "## Let's download the train set first.\n",
    "## and set some usefull values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usefull values\n",
    "DATAPATH = '/mnt/DATA/Python_Projects/google_landmarks_2019/data/'\n",
    "\n",
    "URLTRAINFILE = \"https://s3.amazonaws.com/google-landmark/metadata/train.csv\"\n",
    "TRAINFILE = 'train.csv'\n",
    "\n",
    "URLATTRIBFILE = \"https://s3.amazonaws.com/google-landmark/metadata/train_attribution.csv\"\n",
    "ATTRIBFILE = \"train_attribution.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test file\n",
    "print(os.path.isfile(DATAPATH + TRAINFILE))\n",
    "\n",
    "if os.path.isfile(DATAPATH + TRAINFILE) is False:\n",
    "    download_file(URLTRAINFILE, DATAPATH + TRAINFILE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download attribute file\n",
    "print(os.path.isfile(DATAPATH + TRAINFILE))\n",
    "\n",
    "if os.path.isfile(DATAPATH + ATTRIBFILE) is False:\n",
    "    download_file(URLATTRIBFILE, DATAPATH + ATTRIBFILE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download image files, for a downloaded file, extracts it and deletes the tar one\n",
    "for i in range(100,500):\n",
    "    extension = \"00\" + str(i+1)\n",
    "    extension = extension[-3:]\n",
    "    url = \"https://s3.amazonaws.com/google-landmark/train/images_\" + extension + \".tar\"\n",
    "    filename = \"images_\" + extension + \".tar\"\n",
    "    if os.path.isdir(DATAPATH + filename[:-3]) is False:\n",
    "        if os.path.isfile(DATAPATH + filename) is False:\n",
    "            download_file(url, DATAPATH + filename , False )\n",
    "            print(\"File {} downloaded\".format(filename))\n",
    "        tar = tarfile.open(DATAPATH + filename)\n",
    "        tar.extractall(DATAPATH)\n",
    "        print(\"File {} extracted\".format(filename))\n",
    "        os.remove(DATAPATH + filename)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# It's time to load and beguin to work with data\n",
    "## First loading DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATAPATH + TRAINFILE)\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "print(\"Number of classes {}\".format(len(train.landmark_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib = pd.read_csv(DATAPATH + ATTRIBFILE)\n",
    "print(attrib.head())\n",
    "print(attrib.shape)\n",
    "print(\"Number of classes {}\".format(len(attrib.landmark_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a whopping ~200k landmarks in this dataset. My house is probably also listed here. Now I don't want to use all these 200k categories. Majority of them will have very examples of it. So I'll exclude categories that have less than a certain threshold number of images to it's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THRESHOLD = 250\n",
    "\n",
    "counts = dict(Counter(train['landmark_id']))\n",
    "landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD}\n",
    "NUM_CLASSES = len(landmarks_dict)\n",
    "print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\n",
    "\n",
    "i = 0\n",
    "landmark_to_idx = {}\n",
    "idx_to_landmark = []\n",
    "for k in landmarks_dict:\n",
    "    landmark_to_idx[k] = i\n",
    "    idx_to_landmark.append(k)\n",
    "    i += 1\n",
    "\n",
    "all_urls = train['url'].tolist()\n",
    "all_landmarks = train['landmark_id'].tolist()\n",
    "valid_urls_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict}\n",
    "valid_urls_list = [x[0] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict]\n",
    "\n",
    "NUM_EXAMPLES = len(valid_urls_list)\n",
    "print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check a few sample images from the dataset. First we'll check a few images overall. In the next plot, we'll see how similar are the images from the same categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=20\n",
    "h=20\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "columns = 4\n",
    "rows = 4\n",
    "i = 1\n",
    "for url in valid_urls_list[:16]:\n",
    "    im = download_image_cv2_urllib(url)\n",
    "    if im.size != 0:\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.title(\"Landmark: \"+str(idx_to_landmark[valid_urls_dict[url.split(\"/\")[-1]]]))\n",
    "        plt.imshow(im)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=20\n",
    "h=20\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "columns = 5\n",
    "rows = 4\n",
    "landmarks = [idx_to_landmark[valid_urls_dict[x]] for x in random.sample(valid_urls_dict.keys(), rows)]\n",
    "for i in range(rows):\n",
    "    landmark = landmarks[i]\n",
    "    urls = [x[0] for x in zip(all_urls, all_landmarks) if x[1]==landmark]\n",
    "    for j in range(columns):\n",
    "        im = download_image_cv2_urllib(urls[j])\n",
    "        if im.size != 0:\n",
    "            fig.add_subplot(rows, columns, i*columns+j+1)\n",
    "            plt.title(\"Landmark: \"+str(landmark))\n",
    "            plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is quite a noisy dataset. It is difficult to understand correctly how one image is connected to other in some categories.\n",
    "\n",
    "Not worrying about the noise in the data, let's proceed. Now we will create a train and validation set from the valid examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_urls, validation_urls = train_test_split(valid_urls_list, test_size=1.5*NUM_CLASSES/NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_images = []\n",
    "validation_y = []\n",
    "for url in validation_urls:\n",
    "    im = download_image_cv2_urllib(url)\n",
    "    if im.size != 0:\n",
    "        validation_images.append(im)\n",
    "        validation_y.append(valid_urls_dict[url.split(\"/\")[-1]])\n",
    "\n",
    "valid_x = np.array(validation_images)\n",
    "valid_y = np.zeros((len(validation_images), NUM_CLASSES))\n",
    "        \n",
    "for i in range(len(validation_y)):\n",
    "    valid_y[i,validation_y[i]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGen(Sequence):\n",
    "    def __init__(self, data, batch_size=24, verbose=1):\n",
    "        self.batch_size=batch_size\n",
    "        self.data_urls = data\n",
    "\n",
    "    def normalize(self,data):\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_urls = random.sample(self.data_urls, self.batch_size)\n",
    "        \n",
    "        output = []\n",
    "        y_classes = []\n",
    "        for url in batch_urls:\n",
    "            im = download_image_cv2_urllib(url)\n",
    "            if im.size != 0:\n",
    "                output.append(im)\n",
    "                y_classes.append(valid_urls_dict[url.split(\"/\")[-1]])\n",
    "        \n",
    "        x = np.array(output)\n",
    "        y = np.zeros((len(output), NUM_CLASSES))\n",
    "        \n",
    "        for i in range(len(y_classes)):\n",
    "            y[i,y_classes[i]] = 1.\n",
    "        \n",
    "        return x,y\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        #return len(valid_urls_list) // self.batch_size\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_class(y_true, y_pred):\n",
    "    true = K.argmax(y_true, axis=1)\n",
    "    pred = K.argmax(y_pred, axis=1)\n",
    "    matches = K.equal(true, pred)\n",
    "    return K.mean(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in res.layers[:120]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Flatten()(res.output)\n",
    "out = Dense(NUM_CLASSES, activation='softmax')(out)\n",
    "model = Model(res.input, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "opt = Adam(0.0001)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[accuracy_class])\n",
    "model.fit_generator(generator=DataGen(train_urls, batch_size=128),\n",
    "                    validation_data=[valid_x, valid_y],\n",
    "                    epochs=80,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
